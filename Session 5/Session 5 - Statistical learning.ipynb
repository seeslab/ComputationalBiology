{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last session in the lab\n",
    "\n",
    "In the final lab session, we will be applying the statistical learning concepts and tools that we have learned in the theoretical lectures. In particular, we will go back to the dataset that we used in the first notebook (which contains data about breast cancer) and design algorithms to diagnose whether a given tumor is benign or malignant.\n",
    "\n",
    "The methodology for today's session will be slightly different from previous ones---let us explain this. We started on the first session with the very basics of Python, taking you slowly up to speed by covering thoroughly the most fundamental concepts. Then, throughout the sessions, we introduced more advanced tools so that you needed less and less help from us. First, for example, we discussed how to use the `help()` method to get help on Python classes and functions. Then we started referring to external documentation so that you could learn for yourselves (for example, about [Python dictionaries](https://www.w3schools.com/python/python_dictionaries.asp)). And then, in the final exercise of the 4th lab session, we suggested that you use an alternative method (for simulating single and paired gene knock-outs) that we had not even discussed in the notebook---and **many of you did it**.\n",
    "\n",
    "That is great! **We have gone from (many of you) not knowing any Python at all, to you being able to use methods without us explaining them to you in detail!**\n",
    "\n",
    "That is what the real world is going to look like when you graduate from the university and go out there to work as a biomedical engineer---you will need to check online documentation, ask questions in online forums, and so on. For all of these reasons (because now you are ready to do it, and because it is a useful exercise for you), the last lab session is much less directed than the previous ones. In this session, we will just give you a few pointers about where to get the relevant information, and then we will jump right into the final exercise.\n",
    "\n",
    "Are you ready for the challenge?\n",
    "\n",
    "# Statistical learning using scikit-learn\n",
    "\n",
    "In this final session we will be using [scikit-learn](https://scikit-learn.org/stable/index.html), the major machine learning module for Python. This module is installed by default in Anaconda, so this time you do not need to worry about this. For the final exercise you will be needing tools for: random forests, support vector machines, and cross-validation. Here are a few pointers.\n",
    "\n",
    "## Random forests\n",
    "\n",
    "Random forests (RFs) are powerful and flexible statistical learning methods. The scikit-learn implementation of RFs and their use are described [here](https://scikit-learn.org/stable/modules/ensemble.html#forest). Given a categorical response variable `y` and some features `X`, creating and training a RF in scikit-learn is as easy as this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X = [[0, 0], [1, 1]]\n",
    "y = ['a', 'b']\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `clf` is initialized as a random forest classifier with 10 decision trees, and then it is fit to the training data `X` and `y`. Now, the classifier can be used for prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a'], dtype='<U1')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pred = [[0, .5]]\n",
    "clf.predict(X_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Support vector machines\n",
    "\n",
    "Support vector machines (SVMs) are also powerful models. The training of SVMs and making predictions with them is analogous to RFs, and is described in detail [here](https://scikit-learn.org/stable/modules/svm.html#svm-classification).\n",
    "\n",
    "## Cross validation\n",
    "\n",
    "As we have seen in the theory sessions, if we evaluate the prediction error of a model on the same data that we use to train it, we are likely to overfit and to underestimate the test error. Cross-validation is a technique for evaluating the prediction error of our models robustly. As we have also seen, there are several approaches to cross-validation, notably k-fold cross-validation and leave-one-out cross-validation.\n",
    "\n",
    "Cross-validation in scikit-learn is discussed in detail [here](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation). For the final exercise, we will be using k-fold cross-validation to select the best parameters for our models (for our support vector machines and/or for our random forests). There are basically three ways of doing this: using KFold iterators as discussed [here](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators) and, more specifically, [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold); using cross-validated metrics as described [here](https://scikit-learn.org/stable/modules/cross_validation.html#computing-cross-validated-metrics); or using parameter tuning as discussed [here](https://scikit-learn.org/stable/modules/grid_search.html). These methods are increasingly sophisticated, increasingly difficut to understand, and increasingly easy to use. You will have to choose!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final exercise - Diagnosing malignant breast cancer\n",
    "\n",
    "Here you will consider, again, the [Breast Cancer Wisconsin (Diagnostic) Data Set](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data#data.csv). The features in this data set are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. Each row in the dataset corresponds to a given patient; each column describes a characteristic of the cell nuclei present in the image. Additionally, the *malignant* column specifies whether the tumor is benign (value 0 or `False`) or malignant (value 1 or `True`).  The training and test data for this exercise are available from the files `Files/TRAIN_breast_cancer_kaggle.xls` and `Files/TEST_breast_cancer_kaggle.xls`, respectively. You will be using statistical learning to predict if a given tumor is benign or malignant.\n",
    "\n",
    "1. Load the training breast cancer data into a Pandas data frame (if you do not remember how to do this, check the notebook of Session 1 or read the [Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html) for the `read_excel()` method).\n",
    "\n",
    "2. Store the column corresponding to the response variable *malignant* into a variable named `y`. Store all other columns (corresponding to the features) into a Pandas `DataFrame` variable named `X`, whose columns correspond to features and whose rows correspond to patients, just as in the original Excel file (but without the *malignant* column). Display `X` and `y`.\n",
    "\n",
    "3. Do the same for the test data, building the corresponding `X_test` and `y_test`.\n",
    "\n",
    "4. Explain succinctly whether we are dealing with a regression or a classification problem.\n",
    "\n",
    "5. Create a RF model (for regression or classification, according to your answer in 4) and train it using the training data (`X`, `y`). Now calculate the training accuracy and the test accuracy, that is:\n",
    "    - Use the trained RF to make predictions from the training `X`;\n",
    "    - Evaluate the accuracy of your prediction, that is, the percentage of predictions that coincide exactly with `y` (training accuracy);\n",
    "    - Use the trained RF to make predictions from the test `X_test`;\n",
    "    - Evaluate the accuracy of your prediction, that is, the percentage of predictions that coincide exactly with `y_test` (test accuracy).\n",
    "    - Is the test accuracy larger or smaller than the training accuracy? Why? Which one is a good measure of the predictive power of your model?\n",
    "\n",
    "6. Do the same as in 5, but training a SVM model instead of a RF model.\n",
    "\n",
    "Now, choose the RF model or the SVM model and do the following:\n",
    "\n",
    "7. Use 5-fold cross-validation in the training set (that is, ignoring for now the test set) to calculate validation accuracy. Is the validation accuracy a good estimate of the test accuracy? That is, is the validation accuracy similar to the test accuracy that you calculated earlier in question 5 or 6?\n",
    "\n",
    "8. Use 5-fold cross-validation to optimize the parameters of your model, and calculate the test accuracy using the optimized model. Did it improve with respect to the original test accuracy?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
